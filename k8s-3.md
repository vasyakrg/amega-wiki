---
title: Ceph
description: Запустим ceph-хранилище на основе трех нод
published: true
date: 2019-12-31T07:16:20.759Z
tags: cluster, ceph
---

# Шаг 3
Запускаем Ceph-хранилище, подключаем Dashboard для наглядного мониторинга

Ранее мы уже подготовили три ноды с подключенными к ним отдельными дисками по 10Гб. 
В целом, и более подробно про требования и вообще про Ceph можно почитать на их [сайте](https://docs.ceph.com/docs/jewel/start/hardware-recommendations/)

А мы же начнем подымать тестовое хранилище.
Первое - настроим именования и доступы.
Тут и далее я буду упомянять свои (выданные мне hetzner) адреса, вы - соответственно - использовать свои.

Мои адреса и нейминг, который я хочу им прописать:
```
116.203.249.184 - node1
116.203.249.182 - node2
116.203.249.190 - node3
```

Для начала, научим ноды общаться между собой по ключам.

Идем на каждую ноду по ssh, и задаем там пароль для root, т.к. при создании машин, сгенерированный пароль мы не знаем.

> `ssh root@116.203.249.184`

> `passwd root`

Думаю тут и везде можно использовать пароль. установленный в rancher.
Тоже самое проделываем со второй и третьей нодой.

После задания пароля руту, перекидываем с первой ноды (зайдите снова на нее через ssh) ключи на вторую и третью ноды.
Не забудьте снегерить начальный ключ на первой ноде командой `ssh-keygen`, разумеется без пароля.

> `ssh-copy-id 116.203.249.182`
> `ssh-copy-id 116.203.249.190`

Фраза `Number of key(s) added: 1` после каждой операции говорит о успешном результате.

Теперь надо разобраться с неймингом между серверами.
На первой ноде выполняем команды:
>`echo "116.203.249.184 node1" >> /etc/hosts`
>`echo "116.203.249.182 node2" >> /etc/hosts`
>`echo "116.203.249.190 node3" >> /etc/hosts`

Повторяем эти же команды на второй и третьей ноде.
Теперь пинг по имени ноды на любой из нод должен возвращать положительный результат.

Начинаем установку пакетов и деплой Ceph
Все проделываем находясь на первой ноде

Добавляем отпечаток ключа

> `wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -`

Добавляем пакет деплоя ceph

>`echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list`
`apt update && apt install -y ceph-deploy`

Чтобы не зависеть от железных часов серверов Hetzner, сразу ставим NTP-клиента

> `apt install ntp`

Теперь все операции повторяем на второй и третьей ноде.
Я собрал цельную строку, что бы просто запустить ее одним копипастом:

> `wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add - && echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list && apt update && apt install -y ceph-deploy`


Предварительная подготовка нод закончена, можно собственно поднимать сам кластер
Далее все операции проводим только с первой ноде.

Создаем папку с временными конфигами:

> `mkdir configs && cd configs`
Запускаем деплой кластера

> `ceph-deploy new node1 node2 node3`

Ругнется на отпечатки имени и несоответствие ssh отпечатка - просто yes

Смотрим полученный конфиг

> `cat ceph.conf`

Видим, что имена и их адреса на месте
```
root@node-1:~/configs# cat ceph.conf
[global]
fsid = f0a0e0a3-1dc3-4ba3-afd3-05c9054a23e5
mon_initial_members = node1, node2, node3
mon_host = 116.203.249.184,116.203.249.182,116.203.249.190
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
```

Изначально Ceph подразумевает настройки двух сетей: для данных и приложений, для репликаций. Но мы помним, что мы поднимаем тестовую среду и не будем заморачиваться с настройкой дополнительной сети (хотя у Hetzner это возможно)

Далее, ставим все необходимые зависимости (помним, что все делается с первой ноды)

> ceph-deploy install --release luminous node1 node2 node3

Запускаем мониторы:

> ceph-deploy mon create-initial

И пробрасываем конфигы в каждую ноду

> ceph-deploy admin node1 node2 node3

Проверим, что все хорошо:

> ceph status

И подключим mgr

> ceph-deploy mgr create node1 node2 node3

Обновим конфиг на хостах

> ceph-deploy admin node1 node2 node3



