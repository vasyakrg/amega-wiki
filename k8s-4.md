---
title: k8s + Ceprh
description: Подключаем хранилище в кластер
published: true
date: 2020-01-03T06:51:25.363Z
tags: k8s, kubernetes, ceph
---

# Шаг 4
Теперь, когда кластер работает, а хранилище видит свою емкость, осталось подключить одно к другому

> Сначала работаем со стороны хранилища
{.is-success}

Нужно сгенерить (получать) пароли и создать начальный пул для k8s

Создаем пул для данных с именем `kube` (команду запускаем на node1 хранилища)

> `ceph osd pool create kube 32 32`

**32** и **32** - это `pg` и `pgs`, соответственно. Более точную размерность можно выбрать с помощью [калькулятора](https://ceph.io/pgcalc/), она зависит от размера кластера. В нашем тестовом задании - оставьте как есть.

Теперь создаем подключение и доступы к нему.

> `ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'`

Получаем ключ клиента

> `ceph auth get-key client.kube`

и админиский ключ

> `ceph auth get client.admin 2>&1 |grep "key = " |awk '{print  $3'}`

Со стороны хранилища все, идем в k8s-node1 и дальше будем работать с ней.

> Создаем клиентский и админский секреты, которые мы получили ранее.
{.is-success}


> `echo AQD+1A5eEyN7FhAAWse4c4tumHkNWwdR923Abw== > /tmp/key.client`

> `echo AQAp9QpeKNscGRAACkHk+CTxH93Y44YQ3ozhLQ== > /tmp/key.admin`

> Теперь немного настроим ноду
{.is-success}

что бы работать с кластером из консоли (это можно, точнее даже правильнее, сделать на вашей локальной машине, если бы кластер был рабочим, но т.к. это тест - сделаем настройки на первой ноде самого кластера, что бы потом все прибить и не плодить кучу мусора)

На первой ноде у нас не стоит клиент kubectl
Просаживаем командой

> `snap install kubectl --classic`

И надо еще подключить конфигурацию
Идем в Rancher, в раздел Clusters, кликаем по имени нашего тестового кластера, видим на экране три больших круга с показателями загрузки, а справа сверху кнопку Kubeconfig file. 
Заходим в нее, копируем в буфер код сгенерированного файла (снизу будет удобная ссылка для копирования сразу в буфер)

теперь на первой ноде создаем папку для хранения конфигурации

> `mkdir ~/.kube`

Создаем пустой файл и сохраняем в него конфигурацию

> `vi ~/.kube/config`

Осталось проверить, что все правильно.
Запускаем 

> `kubectl get pods`

В ответ получаем 

```
root@k8s-1:~# kubectl get pods
No resources found in default namespace.
```

Значит все сделано правильно.

> Продолжаем подключение хранилища
{.is-success}

Запихиваем полученные ранее ключи в secrets

> `kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd`

> `kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd`

Результат
```
secret/ceph-secret created
secret/ceph-admin-secret created
```

Теперь нужно научить кластер работать с провизиром ceph rbd provisioner
Клонируем репу

> `git clone https://github.com/kubernetes-incubator/external-storage.git && cd external-storage/ceph/rbd/deploy/`

Задаем неймспейс по умолчанию

> `NAMESPACE=kube-system && sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml`

Запускаем

> `kubectl -n $NAMESPACE apply -f ./rbac`

Вывод должен быть такой
```
clusterrole.rbac.authorization.k8s.io/rbd-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created
deployment.apps/rbd-provisioner created
role.rbac.authorization.k8s.io/rbd-provisioner created
rolebinding.rbac.authorization.k8s.io/rbd-provisioner created
serviceaccount/rbd-provisioner created
```

> Создаем storage class
{.is-success}

**Не забудьте поменять адреса мониторов на свои!**

```
cat <<EOF >./storage-class.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ceph-rbd
provisioner: ceph.com/rbd
parameters:
  monitors: 116.203.249.182:6789, 116.203.249.190:6789, 78.47.145.226:6789
  pool: kube
  adminId: admin
  adminSecretNamespace: kube-system
  adminSecretName: ceph-admin-secret
  userId: kube
  userSecretNamespace: kube-system
  userSecretName: ceph-secret
  imageFormat: "2"
  imageFeatures: layering
EOF
```

Запускаем деплой

> `kubectl apply -f storage-class.yaml`


> С подключеним все
{.is-success}

осталось на каждой ноде просадить поддержку ceph-common
начинаем с первой

> `wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -`

> `echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list`

> `apt update && apt install ceph-common -y`

Так же не забудем подключить **rdb** со стороны ОС

> `modprobe rbd`

Теперь эти же команды проводим на второй и третьей ноде.

> Финал
{.is-success}

Когда поддержка пакетов есть на всех воркер-нодах (а у нас это все три ноды), создаем PersistentVolumeClaim

```
cat <<EOF >./claim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ceph-rbd
  resources:
    requests:
      storage: 1Gi
EOF
``` 

Деплоим

> `kubectl apply -f claim.yaml`

Смотрим, что создалось

> `kubectl get pvc`

Вывод должен быть таким

```
root@k8s-1:~/external-storage/ceph/rbd/deploy# kubectl get pvc
NAME     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim1   Bound    pvc-76b430ab-d80a-4d30-bce7-dc90d7f864f6   1Gi        RWO            ceph-rbd       18s
```

Осталось попробовать создать тестовый под и подключить туда наш волюм

```
cat <<EOF >./create-file-pod.yaml
kind: Pod
apiVersion: v1
metadata:
  name: create-file-pod
spec:
  containers:
  - name: test-pod
    image: gcr.io/google_containers/busybox:1.24
    command:
    - "/bin/sh"
    args:
    - "-c"
    - "echo Hello world! > /mnt/test.txt && exit 0 || exit 1"
    volumeMounts:
    - name: pvc
      mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
  - name: pvc
    persistentVolumeClaim:
      claimName: claim1
EOF
```

Запустим и проверим, что все работает

> `kubectl apply -f create-file-pod.yaml && kubectl get pods -w`

Теперь можно пойти в ранчер и посмотреть, что под создался, выполнился, все зеленое, монтирование диска было успешно.
На этом все :)