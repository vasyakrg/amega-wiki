---
title: k8s + Ceprh
description: Подключаем хранилище в кластер
published: true
date: 2020-01-03T06:09:54.704Z
tags: 
---

# Шаг 4
Теперь, когда кластер работает, а хранилище видит свою емкость, осталось подключить одно к другому

Сначала работаем со стороны хранилища
Нужно сгенерить (получать) пароли и создать начальный пул для k8s

Создаем пул для данных с именем `kube` (команду запускаем на node1 хранилища)

> `ceph osd pool create kube 32 8`

32 и 8 - это `pg` и `pgs`, соответственно. Более точную размерность можно выбрать с помощью [калькулятора](https://ceph.io/pgcalc/), она зависит от размера кластера. В нашем тестовом задании - остаьте как есть.

Теперь создаем подключение и доступы к нему.

> `ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'`

Получаем ключ клиента

> `ceph auth get-key client.kube`

и админиский ключ

> `ceph auth get client.admin 2>&1 |grep "key = " |awk '{print  $3'}`

Со стороны хранилища все, идем в k8s-node1 и дальше будем работать с ней.

Создаем клиентский и админский секреты, которые мы получили ранее.

> echo AQD+1A5eEyN7FhAAWse4c4tumHkNWwdR923Abw== > /tmp/key.client
> echo AQAp9QpeKNscGRAACkHk+CTxH93Y44YQ3ozhLQ== > /tmp/key.admin

Теперь немного настроим ноду, что бы работать с кластером из консоли (это можно, точнее даже правильнее сделать на вашей локальной машине, если бы кластер был рабочим, но т.к. это тест - сделаем настройки на первой ноде самого кластера, что бы потом все прибить и не плодить кучу мусора)

На первой ноде у нас не стоит клиент kubectl
Просаживаем командой

> snap install kubectl --classic

И надо еще подключить конфигурацию
Идем в Rancher, в раздел Clusters, кликаем по имени нашего тестового кластера, видим на экране три больших круга с показателями загрузки, а справа сверху кнопку Kubeconfig file. 
Заходим в нее, копируем в буфер код сгенерированного файла (снизу будет удобная ссылка для копирования сразу в буфер)

теперь на первой ноде создаем папку для хранения конфигурации

> mkdir ~/.kube

Создаем пустой файл и сохраняем в него конфигурацию

> vi ~/.kube/config

Осталось проверить, что все правильно.
Запускаем 

> kubectl get pods

В ответ получаем 

```
root@k8s-1:~# kubectl get pods
No resources found in default namespace.
```

Значит все сделано правильно.
Продолжаем подключение хранилища

Запихиваем полученные ранее ключи в secrets

> kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd
> kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd

Результат
```
secret/ceph-secret created
secret/ceph-admin-secret created
```

Теперь нужно научить кластер работать с провизиром ceph rbd provisioner
Клонируем репу

> git clone https://github.com/kubernetes-incubator/external-storage.git && cd external-storage/ceph/rbd/deploy/

Задаем неймспейс по умолчанию

> NAMESPACE=kube-system && sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml

Запускаем

> kubectl -n $NAMESPACE apply -f ./rbac

Вывод должен быть такой
```
clusterrole.rbac.authorization.k8s.io/rbd-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created
deployment.apps/rbd-provisioner created
role.rbac.authorization.k8s.io/rbd-provisioner created
rolebinding.rbac.authorization.k8s.io/rbd-provisioner created
serviceaccount/rbd-provisioner created
```

Теперь создаем storage class
Не забудьте поменять адреса мониторов на свои!

```
cat <<EOF >./storage-class.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ceph-rbd
provisioner: ceph.com/rbd
parameters:
  monitors: 116.203.249.182:6789, 116.203.249.190:6789, 78.47.145.226:6789
  pool: kube
  adminId: admin
  adminSecretNamespace: kube-system
  adminSecretName: ceph-admin-secret
  userId: kube
  userSecretNamespace: kube-system
  userSecretName: ceph-secret
  imageFormat: "2"
  imageFeatures: layering
EOF
```

Запускаем деплой

> kubectl apply -f storage-class.yaml

С подключем все, осталось на каждой ноде просадить поддержку ceph-common
начинаем с первой

> apt install curl apt-transport-https -y 
> curl https://mirror.croit.io/keys/release.gpg > /usr/share/keyrings/croit-signing-key.gpg && echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' > /etc/apt/sources.list.d/croit-ceph.list
> apt update && apt install ceph-common